# test-set accuracy
knn_pred <- predict(knn_fit, newdata = test)
knn_cm   <- confusionMatrix(knn_pred, test$is_hit)
cat("\nTest accuracy (k-NN):",
round(knn_cm$overall["Accuracy"], 3), "\n")
# Decision tree  (tuning max depth)
# "hit" is the second level, which is what twoClassSummary expects
train$is_hit <- factor(train$is_hit, levels = c("nonhit", "hit"))
test$is_hit <- factor(test$is_hit, levels = c("nonhit", "hit"))
# cross-validation set up
dt_ctrl <- trainControl(
method          = "cv",
number          = 5,
classProbs      = TRUE,
summaryFunction = twoClassSummary,
sampling        = "down",
savePredictions = "final"
)
tree_grid <- expand.grid(
cp = 10 ^ seq(-4, -1, length.out = 10)
)
set.seed(123)
tree_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train,
method = "rpart",      # allows maxdepth tuning
trControl = dt_ctrl,
tuneGrid  = tree_grid,
metric    = "ROC",
control = rpart.control(cp = 0,
minsplit = 2,
minbucket = 1)
)
# best depth and cross-validated accuracy
print(tree_fit$bestTune)
print(max(tree_fit$results$ROC))
prob_test <- predict(tree_fit, test, type = "prob")[ , "hit"]
pred_test <- factor(ifelse(prob_test > 0.5, "hit", "nonhit"),
levels = c("nonhit", "hit"))
cm  <- confusionMatrix(pred_test, test$is_hit)
auc <- roc(test$is_hit, prob_test, levels = c("nonhit", "hit"))$auc
cat("\nTest accuracy :", round(cm$overall["Accuracy"], 3),
"\nTest ROC-AUC  :", round(auc, 3), "\n")
# test-set accuracy
tree_pred <- predict(tree_fit, newdata = test)
tree_cm   <- confusionMatrix(tree_pred, test$is_hit)
cat("\nTest accuracy (Decision tree):",
round(tree_cm$overall["Accuracy"], 3), "\n")
# METHOD 2: categorical classification models
# CROSS-VALIDATION CONTROL
knn_ctrl <- trainControl(method = "cv",
number = 5,
savePredictions = "final")
# kNN  (tuning k)
set.seed(123)
knn_grid <- expand.grid(k = seq(3, 25, by = 2))
knn_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train_small,
method = "knn",
trControl = knn_ctrl,
tuneGrid  = knn_grid,
metric    = "Accuracy")
# best k and cross-validated accuracy
print(knn_fit$bestTune)
print(max(knn_fit$results$Accuracy))
# test-set accuracy
knn_pred <- predict(knn_fit, newdata = test)
knn_cm   <- confusionMatrix(knn_pred, test$is_hit, positive = "1")
# METHOD 2: categorical classification models
# CROSS-VALIDATION CONTROL
knn_ctrl <- trainControl(method = "cv",
number = 5,
savePredictions = "final")
# kNN  (tuning k)
set.seed(123)
knn_grid <- expand.grid(k = seq(3, 25, by = 2))
knn_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train_small,
method = "knn",
trControl = knn_ctrl,
tuneGrid  = knn_grid,
metric    = "Accuracy")
# best k and cross-validated accuracy
print(knn_fit$bestTune)
print(max(knn_fit$results$Accuracy))
# test-set accuracy
knn_pred <- predict(knn_fit, newdata = test)
knn_cm   <- confusionMatrix(knn_pred, test$is_hit, positive = "hit")
cat("\nTest accuracy (k-NN):",
round(knn_cm$overall["Accuracy"], 3), "\n")
# Optional: view the pruned tree
library(rpart.plot)
rpart.plot(tree_fit$finalModel,
type  = 4,      # tidy boxes
extra = 104,    # show class and % of hits
main  = "Best Decision Tree (cp tuned, down-sampled)")
rpart.plot(tree_fit$finalModel, main = "Best Decision Tree")
train_small <- train |>
group_by(is_hit) |>
sample_frac(0.05) |>
ungroup()
# kNN  (tuning k)
set.seed(123)
knn_grid <- expand.grid(k = seq(3, 25, by = 2))
knn_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train_small,
method = "knn",
trControl = knn_ctrl,
tuneGrid  = knn_grid,
metric    = "Accuracy")
# best k and cross-validated accuracy
print(knn_fit$bestTune)
print(max(knn_fit$results$Accuracy))
# test-set accuracy
knn_pred <- predict(knn_fit, newdata = test)
knn_cm   <- confusionMatrix(knn_pred, test$is_hit, positive = "hit")
cat("\nTest accuracy (k-NN):",
round(knn_cm$overall["Accuracy"], 3), "\n")
# Decision tree  (tuning max depth)
# "hit" is the second level, which is what twoClassSummary expects
train$is_hit <- factor(train$is_hit, levels = c("nonhit", "hit"))
test$is_hit <- factor(test$is_hit, levels = c("nonhit", "hit"))
# cross-validation set up
dt_ctrl <- trainControl(
method          = "cv",
number          = 5,
classProbs      = TRUE,
summaryFunction = twoClassSummary,
sampling        = "down",
savePredictions = "final"
)
tree_grid <- expand.grid(
cp = 10 ^ seq(-4, -1, length.out = 10)
)
set.seed(123)
tree_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train,
method = "rpart",      # allows maxdepth tuning
trControl = dt_ctrl,
tuneGrid  = tree_grid,
metric    = "ROC",
control = rpart.control(cp = 0,
minsplit = 2,
minbucket = 1)
)
# best depth and cross-validated accuracy
print(tree_fit$bestTune)
print(max(tree_fit$results$ROC))
prob_test <- predict(tree_fit, test, type = "prob")[ , "hit"]
pred_test <- factor(ifelse(prob_test > 0.5, "hit", "nonhit"),
levels = c("nonhit", "hit"))
cm  <- confusionMatrix(pred_test, test$is_hit)
auc <- roc(test$is_hit, prob_test, levels = c("nonhit", "hit"))$auc
cat("\nTest accuracy :", round(cm$overall["Accuracy"], 3),
"\nTest ROC-AUC  :", round(auc, 3), "\n")
# Optional: view the pruned tree
library(rpart.plot)
rpart.plot(tree_fit$finalModel,
type  = 4,      # tidy boxes
extra = 104,    # show class and % of hits
main  = "Best Decision Tree (cp tuned, down-sampled)")
rpart.plot(tree_fit$finalModel, main = "Best Decision Tree")
train_small <- train |>
group_by(is_hit) |>
sample_frac(0.005) |>
ungroup()
# Decision tree  (tuning max depth)
# "hit" is the second level, which is what twoClassSummary expects
train$is_hit <- factor(train$is_hit, levels = c("nonhit", "hit"))
test$is_hit <- factor(test$is_hit, levels = c("nonhit", "hit"))
# cross-validation set up
dt_ctrl <- trainControl(
method          = "cv",
number          = 5,
classProbs      = TRUE,
summaryFunction = twoClassSummary,
sampling        = "down",
savePredictions = "final"
)
tree_grid <- expand.grid(
cp = 10 ^ seq(-4, -1, length.out = 10)
)
set.seed(123)
tree_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train,
method = "rpart",      # allows maxdepth tuning
trControl = dt_ctrl,
tuneGrid  = tree_grid,
metric    = "ROC",
control = rpart.control(cp = 0,
minsplit = 2,
minbucket = 1)
)
# best depth and cross-validated accuracy
print(tree_fit$bestTune)
print(max(tree_fit$results$ROC))
prob_test <- predict(tree_fit, test, type = "prob")[ , "hit"]
pred_test <- factor(ifelse(prob_test > 0.5, "hit", "nonhit"),
levels = c("nonhit", "hit"))
cm  <- confusionMatrix(pred_test, test$is_hit)
auc <- roc(test$is_hit, prob_test, levels = c("nonhit", "hit"))$auc
cat("\nTest accuracy :", round(cm$overall["Accuracy"], 3),
"\nTest ROC-AUC  :", round(auc, 3), "\n")
# Optional: view the pruned tree
library(rpart.plot)
rpart.plot(tree_fit$finalModel,
type  = 4,      # tidy boxes
extra = 104,    # show class and % of hits
main  = "Best Decision Tree (cp tuned, down-sampled)")
train_small <- train |>
group_by(is_hit) |>
sample_frac(0.20) |>
ungroup()
# METHOD 2: categorical classification models
# CROSS-VALIDATION CONTROL
knn_ctrl <- trainControl(method = "cv",
number = 5,
savePredictions = "final")
# kNN  (tuning k)
set.seed(123)
knn_grid <- expand.grid(k = seq(3, 25, by = 2))
knn_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train_small,
method = "knn",
trControl = knn_ctrl,
tuneGrid  = knn_grid,
metric    = "Accuracy")
# best k and cross-validated accuracy
print(knn_fit$bestTune)
print(max(knn_fit$results$Accuracy))
# test-set accuracy
knn_pred <- predict(knn_fit, newdata = test)
knn_cm   <- confusionMatrix(knn_pred, test$is_hit, positive = "hit")
cat("\nTest accuracy (k-NN):",
round(knn_cm$overall["Accuracy"], 3), "\n")
# Decision tree  (tuning max depth)
# "hit" is the second level, which is what twoClassSummary expects
train$is_hit <- factor(train$is_hit, levels = c("nonhit", "hit"))
test$is_hit <- factor(test$is_hit, levels = c("nonhit", "hit"))
# cross-validation set up
dt_ctrl <- trainControl(
method          = "cv",
number          = 5,
classProbs      = TRUE,
summaryFunction = twoClassSummary,
sampling        = "down",
savePredictions = "final"
)
tree_grid <- expand.grid(
cp = 10 ^ seq(-4, -1, length.out = 10)
)
set.seed(123)
tree_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train,
method = "rpart",      # allows maxdepth tuning
trControl = dt_ctrl,
tuneGrid  = tree_grid,
metric    = "ROC",
control = rpart.control(cp = 0,
minsplit = 2,
minbucket = 1)
)
# best depth and cross-validated accuracy
print(tree_fit$bestTune)
print(max(tree_fit$results$ROC))
prob_test <- predict(tree_fit, test, type = "prob")[ , "hit"]
pred_test <- factor(ifelse(prob_test > 0.5, "hit", "nonhit"),
levels = c("nonhit", "hit"))
cm  <- confusionMatrix(pred_test, test$is_hit)
auc <- roc(test$is_hit, prob_test, levels = c("nonhit", "hit"))$auc
cat("\nTest accuracy :", round(cm$overall["Accuracy"], 3),
"\nTest ROC-AUC  :", round(auc, 3), "\n")
# Optional: view the pruned tree
library(rpart.plot)
rpart.plot(tree_fit$finalModel,
type  = 4,      # tidy boxes
extra = 104,    # show class and % of hits
main  = "Best Decision Tree (cp tuned, down-sampled)")
ggplot(knn_fit$results, aes(k, Accuracy)) +
geom_line() +
geom_point(size = 2) +
geom_vline(xintercept = knn_fit$bestTune$k,
linetype = "dashed") +
labs(title = "Cross-validated accuracy across k values",
x = "Number of neighbours (k)",
y = "Accuracy") +
theme_minimal()
cm_df <- as.data.frame(knn_cm$table)
names(cm_df) <- c("Predicted", "Actual", "Freq")
ggplot(cm_df, aes(Actual, Predicted, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), colour = "white", size = 5) +
scale_fill_gradient(low = "skyblue", high = "steelblue") +
theme_minimal() +
labs(title = "Confusion matrix for test data")
# Decision tree  (tuning max depth)
# "hit" is the second level, which is what twoClassSummary expects
train$is_hit <- factor(train$is_hit, levels = c("nonhit", "hit"))
test$is_hit <- factor(test$is_hit, levels = c("nonhit", "hit"))
# 1. Build a PCA for visualisation only
pcs <- prcomp(train_small %>%
select(danceability, energy, loudness, valence,
tempo, speechiness, liveness, instrumentalness,
acousticness, duration_ms),
scale. = TRUE)
train_pca <- as_tibble(pcs$x[, 1:2]) %>%
mutate(is_hit = train_small$is_hit)
# 2. Train a small k-NN on those two PCs
set.seed(123)
pca_knn <- train(is_hit ~ ., data = train_pca,
method = "knn",
tuneGrid = data.frame(k = knn_fit$bestTune$k))
# 3. Predict on a grid for a smooth boundary
grid <- expand.grid(PC1 = seq(min(train_pca$PC1), max(train_pca$PC1), length = 200),
PC2 = seq(min(train_pca$PC2), max(train_pca$PC2), length = 200))
grid$pred <- predict(pca_knn, newdata = grid)
ggplot() +
geom_tile(data = grid, aes(PC1, PC2, fill = pred), alpha = 0.3) +
geom_point(data = train_pca, aes(PC1, PC2, colour = is_hit), size = 1) +
scale_fill_manual(values = c("0" = "grey80", "1" = "lightpink")) +
scale_colour_manual(values = c("0" = "grey20", "1" = "red3")) +
labs(title = "k-NN decision regions in PC space",
fill = "Predicted",
colour = "Actual") +
theme_minimal()
# cross-validation set up
dt_ctrl <- trainControl(
method          = "cv",
number          = 5,
classProbs      = TRUE,
summaryFunction = twoClassSummary,
sampling        = "down",
savePredictions = "final"
)
tree_grid <- expand.grid(
cp = 10 ^ seq(-4, -1, length.out = 10)
)
set.seed(123)
tree_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train,
method = "rpart",      # allows maxdepth tuning
trControl = dt_ctrl,
tuneGrid  = tree_grid,
metric    = "ROC",
control = rpart.control(cp = 0,
minsplit = 2,
minbucket = 1)
)
# 1. Build a PCA for visualisation only
pcs <- prcomp(train_small %>%
select(danceability, energy, loudness, valence,
tempo, speechiness, liveness, instrumentalness,
acousticness, duration_ms),
scale. = TRUE)
# 1. Build a PCA for visualisation only
pcs <- prcomp(train_small %>%
select(danceability, energy, loudness, valence,
tempo, speechiness, liveness, instrumentalness,
acousticness, duration_ms),
scale. = TRUE)
train_pca <- as_tibble(pcs$x[, 1:2]) %>%
mutate(is_hit = train_small$is_hit)
# 2. Train a small k-NN on those two PCs
set.seed(123)
pca_knn <- train(is_hit ~ ., data = train_pca,
method = "knn",
tuneGrid = data.frame(k = knn_fit$bestTune$k))
# 3. Predict on a grid for a smooth boundary
grid <- expand.grid(PC1 = seq(min(train_pca$PC1), max(train_pca$PC1), length = 200),
PC2 = seq(min(train_pca$PC2), max(train_pca$PC2), length = 200))
grid$pred <- predict(pca_knn, newdata = grid)
ggplot() +
geom_tile(data = grid, aes(PC1, PC2, fill = pred), alpha = 0.3) +
geom_point(data = train_pca, aes(PC1, PC2, colour = is_hit), size = 1) +
scale_fill_manual(values = c("0" = "grey80", "1" = "lightpink")) +
scale_colour_manual(values = c("0" = "grey20", "1" = "red3")) +
labs(title = "k-NN decision regions in PC space",
fill = "Predicted",
colour = "Actual") +
theme_minimal()
cm_df <- as.data.frame(knn_cm$table)
names(cm_df) <- c("Predicted", "Actual", "Freq")
ggplot(cm_df, aes(Actual, Predicted, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), colour = "white", size = 5) +
scale_fill_gradient(low = "skyblue", high = "steelblue") +
theme_minimal() +
labs(title = "Confusion matrix for test data")
ggplot(knn_fit$results, aes(k, Accuracy)) +
geom_line() +
geom_point(size = 2) +
geom_vline(xintercept = knn_fit$bestTune$k,
linetype = "dashed") +
labs(title = "Cross-validated accuracy across k values",
x = "Number of neighbours (k)",
y = "Accuracy") +
theme_minimal()
pdf("decision_tree.pdf", width = 30, height = 15)
rpart.plot(
tree_fit$finalModel,
type  = 4,
extra = 104,
main  = "Best Decision Tree (cp tuned, down-sampled)"
)
dev.off()
pdf("decision_tree.pdf", width = 30, height = 15)
rpart.plot(
tree_fit$finalModel,
type  = 4,
extra = 104,
main  = "Best Decision Tree (cp tuned, down-sampled)"
)
dev.off()
library(rpart.plot)
rpart.plot(tree_fit$finalModel,
type  = 4,      # tidy boxes
extra = 104,    # show class and % of hits
main  = "Best Decision Tree (cp tuned, down-sampled)")
pdf("decision_tree.pdf", width = 30, height = 15)
rpart.plot(
tree_fit$finalModel,
type  = 4,
extra = 104,
main  = "Best Decision Tree (cp tuned, down-sampled)"
)
dev.off()
library(partykit)
install.packages("partykit")
library(partykit)
party_tree <- as.party(tree_fit$finalModel)
plot(party_tree, tp_args = list(id = FALSE))
install.packages("rattle")
library(rattle)
fancyRpartPlot(tree_fit$finalModel)
library(rattle)
fancyRpartPlot(tree_fit$finalModel)
View(rpart.plot(tree_fit$finalModel, main = "Best Decision Tree"))
View(fancyRpartPlot(tree_fit$finalModel))
fancyRpartPlot(tree_fit$finalModel)
# install.packages(c("partykit", "data.tree", "DiagrammeR"))
library(partykit)    # for as.party()
library(data.tree)   # for as.Node()
install.packages("data.tree")
install.packages("DiagrammeR")
# install.packages(c("partykit", "data.tree", "DiagrammeR"))
library(partykit)    # for as.party()
library(data.tree)   # for as.Node()
library(DiagrammeR)  # for render_graph()
# 1. turn your rpart into a party object
p_tree <- as.party(tree_fit$finalModel)
# 2. convert that into a data.tree structure
dt_tree <- as.Node(p_tree)
# 3. build a DiagrammeR graph from the data.tree
dg    <- ToDiagrammeRGraph(dt_tree)
# 4. render in Viewer
render_graph(dg)
cm_df <- as.data.frame(knn_cm$table)
names(cm_df) <- c("Predicted", "Actual", "Freq")
ggplot(cm_df, aes(Actual, Predicted, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), colour = "white", size = 5) +
scale_fill_gradient(low = "skyblue", high = "steelblue") +
theme_minimal() +
labs(title = "Confusion matrix for test data")
# METHOD 2: categorical classification models
# CROSS-VALIDATION CONTROL
knn_ctrl <- trainControl(method = "cv",
number = 5,
savePredictions = "final")
# kNN  (tuning k)
set.seed(123)
knn_grid <- expand.grid(k = seq(3, 25, by = 2))
knn_fit <- train(is_hit ~ danceability + energy + loudness + valence +
tempo + speechiness + liveness + instrumentalness +
acousticness + duration_ms,
data = train_small,
method = "knn",
trControl = knn_ctrl,
tuneGrid  = knn_grid,
metric    = "Accuracy")
# best k and cross-validated accuracy
print(knn_fit$bestTune)
print(max(knn_fit$results$Accuracy))
# test-set accuracy
knn_pred <- predict(knn_fit, newdata = test)
knn_cm   <- confusionMatrix(knn_pred, test$is_hit, positive = "hit")
cat("\nTest accuracy (k-NN):",
round(knn_cm$overall["Accuracy"], 3), "\n")
library(dplyr)
library(tidyr)
library(ggplot2)
library(FactoMineR)   # For PCA computation
library(factoextra)   # For PCA visualization
library(cluster)
spotify_data <- read.csv("~/Downloads/SpotifyFeatures.csv")
